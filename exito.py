import streamlit as st
import requests
from urllib.parse import urlparse
import re
import json
import matplotlib.pyplot as plt
import io

# Funci√≥n para extraer subdominios
def extraer_subdominios(domain, api_key):
    url = "https://google.serper.dev/search"
    payload = json.dumps({
        "q": f"site:*.{domain} -site:www.{domain}",
        "num": 100
    })
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json'
    }
    response = requests.request("POST", url, headers=headers, data=payload)
    response.raise_for_status()
    
    results = response.json().get('organic', [])
    subdominios = set()
    for result in results:
        parsed_url = urlparse(result['link'])
        if parsed_url.netloc.endswith(domain) and parsed_url.netloc != domain:
            subdominios.add(parsed_url.netloc)
    
    return list(subdominios)

# Funci√≥n para obtener b√∫squeda de Serper
def obtener_busqueda_serper(query, api_key):
    url = "https://google.serper.dev/search"
    payload = json.dumps({
        "q": query,
        "num": 10
    })
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json'
    }
    response = requests.request("POST", url, headers=headers, data=payload)
    response.raise_for_status()
    
    results = response.json()
    summary = ""
    for result in results.get('organic', []):
        summary += f"{result['title']}\n{result['snippet']}\n\n"
    
    return summary

# Funci√≥n actualizada para obtener an√°lisis de Together
def obtener_analisis_together(summary, api_key):
    url = "https://api.together.xyz/v1/completions"
    payload = {
        "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",  # Se ha cambiado el modelo aqu√≠
        "prompt": f"Human: Analiza el siguiente resumen y proporciona un an√°lisis detallado del potencial de √©xito de la plataforma digital, incluyendo recomendaciones para mejorar y una estimaci√≥n del m√°ximo de visitantes diarios:\n\n{summary}\n\nAssistant: Basado en el resumen proporcionado, aqu√≠ est√° mi an√°lisis detallado del potencial de √©xito de la plataforma digital:",
        "max_tokens": 1000,
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "repetition_penalty": 1,
        "stop": ["Human:", "Assistant:"]
    }
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    response = requests.post(url, json=payload, headers=headers)
    response.raise_for_status()
    
    return response.json()['choices'][0]['text'].strip()

# Funci√≥n para generar gr√°ficas
def generar_graficas(secciones):
    plots = {}
    for titulo, contenido in secciones.items():
        if titulo == "Estimaci√≥n de Visitantes Diarios":
            # Extraer el n√∫mero de visitantes del contenido
            match = re.search(r'(\d+(?:,\d+)?)', contenido)
            if match:
                visitantes = int(match.group(1).replace(',', ''))
                fig, ax = plt.subplots()
                ax.bar(['Estimaci√≥n'], [visitantes])
                ax.set_ylabel('N√∫mero de Visitantes')
                ax.set_title('Estimaci√≥n de Visitantes Diarios')
                
                # Guardar la figura en un buffer
                buf = io.BytesIO()
                plt.savefig(buf, format='png')
                buf.seek(0)
                plots[titulo] = buf
    
    return plots

# Funci√≥n principal
def main():
    # T√≠tulo de la aplicaci√≥n
    st.title("üìà An√°lisis de Potencial de √âxito de Plataformas Digitales")

    # Descripci√≥n de la aplicaci√≥n
    st.markdown("""
    Esta aplicaci√≥n analiza el potencial de √©xito de una plataforma digital basada en su URL. Utiliza las APIs de Serper para obtener informaci√≥n relevante sobre la plataforma y de Together para evaluar su potencial en el mercado actual. Adem√°s, proporciona recomendaciones detalladas para mejorar tanto en forma como en contenido, as√≠ como una estimaci√≥n del m√°ximo de visitantes diarios que puede recibir la plataforma.

    **Funcionalidades Adicionales:**
    - **An√°lisis de Subdominios:** La aplicaci√≥n identificar√° y analizar√° autom√°ticamente todos los subdominios asociados al dominio principal proporcionado.
    """)

    # Entrada de la URL
    url_input = st.text_input("üîó Ingresa la URL de la plataforma digital que deseas analizar:", "")

    # Bot√≥n para iniciar el an√°lisis
    if st.button("‚úÖ Analizar"):
        if not url_input:
            st.error("‚ö†Ô∏è Por favor, ingresa una URL v√°lida.")
        else:
            try:
                # Validar y formatear la URL
                parsed_url = urlparse(url_input)
                if not parsed_url.scheme:
                    url_input = "https://" + url_input  # Preferir HTTPS
                    parsed_url = urlparse(url_input)

                domain = parsed_url.netloc
                if not domain:
                    st.error("‚ö†Ô∏è URL inv√°lida. Por favor, intenta nuevamente.")
                    st.stop()

                # Mostrar la URL que se analizar√°
                st.write(f"**URL a analizar:** {url_input}")

                # Verificar acceso a la URL
                try:
                    response = requests.get(url_input, timeout=10)
                    response.raise_for_status()
                except requests.exceptions.HTTPError as http_err:
                    st.error(f"‚ö†Ô∏è Error HTTP al acceder a la URL: {http_err}")
                    st.stop()
                except requests.exceptions.ConnectionError:
                    st.error("‚ö†Ô∏è Error de conexi√≥n. Verifica tu red y la URL ingresada.")
                    st.stop()
                except requests.exceptions.Timeout:
                    st.error("‚ö†Ô∏è Tiempo de espera excedido al intentar acceder a la URL.")
                    st.stop()
                except requests.exceptions.RequestException as e:
                    st.error(f"‚ö†Ô∏è Error al acceder a la URL: {e}")
                    st.stop()

                st.info("üîÑ Procesando la URL...")

                # Realizar b√∫squeda con Serper API para obtener subdominios
                serper_api_key = st.secrets["serper_api_key"]
                with st.spinner("üîç Buscando subdominios con Serper..."):
                    try:
                        subdominios = extraer_subdominios(domain, serper_api_key)
                    except requests.exceptions.HTTPError as http_err:
                        st.error(f"‚ùå Error HTTP al acceder a Serper API: {http_err}")
                        st.stop()
                    except requests.exceptions.RequestException as e:
                        st.error(f"‚ùå Error al acceder a Serper API: {e}")
                        st.stop()

                if subdominios:
                    st.success(f"‚úÖ Se encontraron {len(subdominios)} subdominio(s).")
                else:
                    st.warning("‚ö†Ô∏è No se encontraron subdominios para analizar.")

                # Agregar el dominio principal a la lista de subdominios para an√°lisis
                todos_los_dominios = [domain] + subdominios

                # Preparar una estructura para almacenar los an√°lisis
                analisis_resultados = {}

                for dom in todos_los_dominios:
                    st.write(f"### üîç **Analizando:** {dom}")
                    # Realizar b√∫squeda con Serper API para obtener informaci√≥n del dominio
                    query = f"Informaci√≥n sobre {dom}"
                    with st.spinner(f"üß† Analizando {dom} con Together..."):
                        try:
                            search_summary = obtener_busqueda_serper(query, serper_api_key)
                            if not search_summary:
                                st.warning(f"‚ö†Ô∏è No se encontr√≥ informaci√≥n relevante para {dom}.")
                                analisis_resultados[dom] = "No se encontr√≥ informaci√≥n relevante."
                                continue
                            analysis = obtener_analisis_together(search_summary, st.secrets["together_api_key"])
                            analisis_resultados[dom] = analysis
                        except requests.exceptions.HTTPError as http_err:
                            st.error(f"‚ùå Error HTTP al acceder a la API: {http_err}")
                            st.error(f"Detalles de la respuesta: {http_err.response.text}")
                            analisis_resultados[dom] = "Error en el an√°lisis."
                            continue
                        except requests.exceptions.RequestException as e:
                            st.error(f"‚ùå Error al acceder a la API: {e}")
                            analisis_resultados[dom] = "Error en el an√°lisis."
                            continue
                        except ValueError as ve:
                            st.error(f"‚ùå {ve}")
                            analisis_resultados[dom] = "Error en el an√°lisis."
                            continue
                        except Exception as e:
                            st.error(f"‚ùå Error inesperado: {str(e)}")
                            analisis_resultados[dom] = "Error en el an√°lisis."
                            continue

                    # Procesar y unificar el an√°lisis
                    # Separar el an√°lisis en secciones utilizando t√≠tulos en negrita
                    secciones = {}
                    current_section = None
                    for line in analysis.split('\n'):
                        line = line.strip()
                        match = re.match(r'\*\*(.*?)\*\*:', line)
                        if match:
                            section_title = match.group(1).strip()
                            secciones[section_title] = ""
                            current_section = section_title
                        elif current_section:
                            secciones[current_section] += line + "\n"

                    # Manejar caso sin secciones
                    if not secciones:
                        secciones["Contenido"] = analysis

                    # Generar las gr√°ficas y obtener los buffers de imagen
                    plots = generar_graficas(secciones)

                    # Mostrar el an√°lisis para el dominio/subdominio
                    with st.container():
                        st.subheader("üìä An√°lisis Unificado")
                        for titulo, contenido in secciones.items():
                            st.markdown(f"### üìå **{titulo}**")
                            st.write(contenido)

                            # Incluir visualizaciones dentro de las secciones pertinentes
                            if titulo in plots:
                                # Mostrar la gr√°fica en Streamlit
                                st.image(plots[titulo], use_column_width=True)

                                # Agregar explicaci√≥n detallada para la estimaci√≥n de visitantes diarios
                                if titulo == "Estimaci√≥n de Visitantes Diarios":
                                    st.markdown("""
                                    **¬øC√≥mo se estima el n√∫mero m√°ximo de visitantes diarios?**
                                    Esta estimaci√≥n se basa en el an√°lisis de la informaci√≥n recopilada sobre la plataforma,
                                    incluyendo factores como el tr√°fico actual, el nicho de mercado, la competencia y el potencial de crecimiento.
                                    Es una proyecci√≥n aproximada y puede variar seg√∫n diversos factores externos.
                                    """)

            except Exception as e:
                st.error(f"‚ùå Se produjo un error inesperado: {str(e)}")

if __name__ == "__main__":
    main()
